<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>test</title>
    <link href="/2022/06/19/test/"/>
    <url>/2022/06/19/test/</url>
    
    <content type="html"><![CDATA[<p><img src="/pic/chunk.png" alt="img"><br><img src="/chunk.png" alt="img"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>详解kaldi中的egs -- 以pybind分支为例</title>
    <link href="/2022/06/01/egs_in_kaldi/"/>
    <url>/2022/06/01/egs_in_kaldi/</url>
    
    <content type="html"><![CDATA[<h1 id="详解kaldi中的egs-–-以pybind分支为例"><a href="#详解kaldi中的egs-–-以pybind分支为例" class="headerlink" title="详解kaldi中的egs – 以pybind分支为例"></a>详解kaldi中的egs – 以pybind分支为例</h1><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><blockquote><div class="code-wrapper"><pre><code class="hljs">   egs是Kaldi中定义的训练所需的样本存档，以chain模型为例，它由由chain_lib中的generate_chain_egs()函数生成，生成egs之后，调用chain_lib中的prepare_initial_acoustic_model()函数，生成初始的模型，最后通过train_one_iteration()函数对模型进行训练。</code></pre></div></blockquote><h3 id="1-egs组成示例"><a href="#1-egs组成示例" class="headerlink" title="1. egs组成示例"></a>1. egs组成示例</h3><h4 id="kaldi-chain模型中的egs"><a href="#kaldi-chain模型中的egs" class="headerlink" title="kaldi chain模型中的egs"></a>kaldi chain模型中的egs</h4><blockquote><p>AISHU_CONVERSATION_P8_F2F-CN-2018-04-18_00794_A_00653-117 <Nnet3ChainEg> <NumInputs> 2<br> 2 <NnetIo> input <I1V> 194 <I1> 0 -22 0 <I1> 0 -21 0 <I1> 0 -20 0 <I1> 0 -19 0 <I1> 0 -18 0 <I1> 0 -17 0 &lt;      I1&gt; 0 -16 0 <I1> 0 -15 0 <I1> 0 -14 0 <I1> 0 -13 0 <I1> 0 -12 0 <I1> 0 -11 0 <I1> 0 -10 0 <I1> 0 -9 0 &lt;I      1&gt; 0 -8 0 <I1> 0 -7 0 <I1> 0 -6 0 <I1> 0 -5 0 <I1> 0 -4 0 <I1> 0 -3 0 <I1> 0 -2 0 <I1> 0 -1 0 <I1> 0 0 0       <I1> 0 1 0 <I1> 0 2 0 <I1> 0 3 0 <I1> 0 4 0 <I1> 0 5 0 <I1> 0 6 0 <I1> 0 7 0 <I1> 0 8 0 <I1> 0 9 0 <I1>       0 10 0 <I1> 0 11 0 <I1> 0 12 0 <I1> 0 13 0 <I1> 0 14 0 <I1> 0 15 0 <I1> 0 16 0 <I1> 0 17 0 <I1> 0 18 0       <I1> 0 19 0 <I1> 0 20 0 <I1> 0 21 0 <I1> 0 22 0 <I1> 0 23 0 <I1> 0 24 0 <I1> 0 25 0 <I1> 0 26 0 <I1> 0 2      7 0 <I1> 0 28 0 <I1> 0 29 0 <I1> 0 30 0 <I1> 0 31 0 <I1> 0 32 0 <I1> 0 33 0 <I1> 0 34 0 <I1> 0 35 0 <I1>       0 36 0 <I1> 0 37 0 <I1> 0 38 0 <I1> 0 39 0 <I1> 0 40 0 <I1> 0 41 0 <I1> 0 42 0 <I1> 0 43 0 <I1> 0 44 0       <I1> 0 45 0 <I1> 0 46 0 <I1> 0 47 0 <I1> 0 48 0 <I1> 0 49 0 <I1> 0 50 0 <I1> 0 51 0 <I1> 0 52 0 <I1> 0 5      3 0 <I1> 0 54 0 <I1> 0 55 0 <I1> 0 56 0 <I1> 0 57 0 <I1> 0 58 0 <I1> 0 59 0 <I1> 0 60 0 <I1> 0 61 0 <I1>       0 62 0 <I1> 0 63 0 <I1> 0 64 0 <I1> 0 65 0 <I1> 0 66 0 <I1> 0 67 0 <I1> 0 68 0 <I1> 0 69 0 <I1> 0 70 0       <I1> 0 71 0 <I1> 0 72 0 <I1> 0 73 0 <I1> 0 74 0 <I1> 0 75 0 <I1> 0 76 0 <I1> 0 77 0 <I1> 0 78 0 <I1> 0 7      9 0 <I1> 0 80 0 <I1> 0 81 0 <I1> 0 82 0 <I1> 0 83 0 <I1> 0 84 0 <I1> 0 85 0 <I1> 0 86 0 <I1> 0 87 0 <I1>       0 88 0 <I1> 0 89 0 <I1> 0 90 0 <I1> 0 91 0 <I1> 0 92 0 <I1> 0 93 0 <I1> 0 94 0 <I1> 0 95 0 <I1> 0 96 0       <I1> 0 97 0 <I1> 0 98 0 <I1> 0 99 0 <I1> 0 100 0 <I1> 0 101 0 <I1> 0 102 0 <I1> 0 103 0 <I1> 0 104 0 <I1      > 0 105 0 <I1> 0 106 0 <I1> 0 107 0 <I1> 0 108 0 <I1> 0 109 0 <I1> 0 110 0 <I1> 0 111 0 <I1> 0 112 0 <I1> 0 113 0 <I1> 0 114 0 <I1> 0 115 0 <I1> 0 116 0 <I1> 0 117 0 <I1> 0 118 0 <I1> 0 119 0 <I1> 0 120 0 <I1> 0 121 0 <I1> 0 122 0 <I1> 0 123 0 <I1> 0 124 0 <I1> 0 125 0 <I1> 0 126 0 <I1> 0 127 0 <I1> 0 128 0 <I1> 0 129 0 <I1> 0 130 0 <I1> 0 131 0 <I1> 0 132 0 <I1> 0 133 0 <I1> 0 134 0 <I1> 0 135 0 <I1> 0 136 0 <I1> 0 137 0 <I1> 0 138 0 <I1> 0 139 0 <I1> 0 140 0 <I1> 0 141 0 <I1> 0 142 0 <I1> 0 143 0 <I1> 0 144 0 <I1> 0 145 0 <I1> 0 146 0 <I1> 0 147 0 <I1> 0 148 0 <I1> 0 149 0 <I1> 0 150 0 <I1> 0 151 0 <I1> 0 152 0 <I1      > 0 153 0 <I1> 0 154 0 <I1> 0 155 0 <I1> 0 156 0 <I1> 0 157 0 <I1> 0 158 0 <I1> 0 159 0 <I1> 0 160 0 <I1      > 0 161 0 <I1> 0 162 0 <I1> 0 163 0 <I1> 0 164 0 <I1> 0 165 0 <I1> 0 166 0 <I1> 0 167 0 <I1> 0 168 0 <I1      > 0 169 0 <I1> 0 170 0 <I1> 0 171 0  [</p><p>(194*80) ]</p></NnetIo><NnetIo> ivector <I1V> 1 <I1> 0 0 0  [(1\*100) ]</NnetIo><NumOutputs> 1<NnetChainSup> output <I1V> 50 <I1> 0 0 0 <I1> 0 3 0 <I1> 0 6 0 <I1> 0 9 0 <I1> 0 12 0 <I1> 0 15 0 <I1> 0 18 0 <I1> 0 21 0 <I1> 0 24 0 <I1> 0 27 0 <I1> 0 30 0 <I1> 0 33 0 <I1> 0 36 0 <I1> 0 39 0 <I1> 0 42 0 <I1> 0 45 0 <I1> 0 48 0 <I1> 0 51 0 <I1> 0 54 0 <I1> 0 57 0 <I1> 0 60 0 <I1> 0 63 0 <I1> 0 66 0 <I1> 0 69 0 <I1> 0 72 0 <I1> 0 75 0 <I1> 0 78 0 <I1> 0 81 0 <I1> 0 84 0 <I1> 0 87 0 <I1> 0 90 0 <I1> 0 93 0 <I1> 0 96 0 <I1> 0 99 0 <I1> 0 102 0 <I1> 0 105 0 <I1> 0 108 0 <I1> 0 111 0 <I1> 0 114 0 <I1> 0 117 0 <I1> 0 120 0 <I1> 0 123 0 <I1> 0 126 0 <I1> 0 129 0 <I1> 0 132 0 <I1> 0 135 0 <I1> 0 138 0 <I1> 0 141 0 <I1> 0 144 0 <I1> 0 147 0 <Supervision> <Weight> 1 <NumSequences> 1 <FramesPerSeq> 50 <LabelDim> 4848 <End2End> T <Fsts>0       10      1       1       50.405240       10      87      87      50.40450       3       121     121     48.433710       15      166     166     49.71114...........<p></Fsts> <AlignmentPdfs> [ 205 2204 83 91 1 223 158 357 878 878 922 1219 1219 1219 1219 1913 1945 267 376 376 702 28 1381 1381 1381 1381 503 590 653 1391 1391 1391 1391 1391 28 503 590 590 0 86 90 155 155 28 1381 1381 503 590 590 590 ]<br></Supervision> <DW2>  [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ]<br></NnetChainSup>                           </p></blockquote><h4 id="pybind-chain模型中的egs"><a href="#pybind-chain模型中的egs" class="headerlink" title="pybind chain模型中的egs"></a>pybind chain模型中的egs</h4><p>原音频：BAC009S0002W0122  这步将音频分成多个chunk</p><blockquote><p>BAC009S0002W0122-0-29-150-29-v1 <Nnet3ChainEg> <NumInputs> 1<br><NnetIo> input <I1V> 208 <I1> 0 -29 0 <I1> 0 -28 0 <I1> 0 -27 0 <I1> 0 -26 0 <I1> 0 -25 0 <I1> 0 -24 0 <I1> 0 -23 0 <I1> 0 -22 0 &lt;I       1&gt; 0 -21 0 <I1> 0 -20 0 <I1> 0 -19 0 <I1> 0 -18 0 <I1> 0 -17 0 <I1> 0 -16 0 <I1> 0 -15 0 <I1> 0 -14 0 <I1> 0 -13 0 <I1> 0 -12 0 <I1> 0 -11 0 <I1> 0 -10 0 <I1> 0 -9 0 </p><p><strong>….</strong></p><p>177 0 <I1> 0 178 0  [</p><p>(208*120)行数据    ]</p></NnetIo><p><NumOutputs> 1</p><p><NnetChainSup> output <I1V> 50 <I1> 0 0 0 <I1> 0 3 0 <I1> 0 6 0 <I1> 0 9 0 <I1> 0 12 0 <I1> 0 15 0 <I1> 0 18 0 <I1> 0 21 0 <I1> 0 2       4 0 <I1> 0 27 0 <I1> 0 30 0 <I1> 0 33 0 <I1> 0 36 0 <I1> 0 <strong>….</strong></p><p>111 0 <I1> 0 114 0 <I1> 0 117 0 <I1> 0 120        0 <I1> 0 123 0 <I1> 0 126 0 <I1> 0 129 0 <I1> 0 132 0 <I1> 0 135 0 <I1> 0 138 0 <I1> 0 141 0 <I1> 0 144 0 <I1> 0 147 0 <Supervision> <Weight> 1 <NumSequences> 1 <FramesPerSeq> 50 <LabelDim> 3480 <End2End> F<br>0       1       1       1       52.49311<br>1       2       68      68<br>2       3       68      68<br>3       4       68      68<br>4       5       68      68<br>5       6       68      68<br>6       7       68      68<br>7       8       68      68<br>8       9       68      68<br>9       10      68      68<br>10      11      68      68</p><p>…</p><p>…</p><p></Supervision> <DW2>  [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ]<br></NnetChainSup></p></blockquote><h3 id="2-egs的可视化方法与构成"><a href="#2-egs的可视化方法与构成" class="headerlink" title="2. egs的可视化方法与构成"></a>2. egs的可视化方法与构成</h3><h5 id="1-可视化方法"><a href="#1-可视化方法" class="headerlink" title="1. 可视化方法"></a>1. 可视化方法</h5><p>在chain模型中利用<code>nnet3-chain-copy-egs</code>将ark文件转换成txt文件进行可视化。</p><h5 id="2-内容构成"><a href="#2-内容构成" class="headerlink" title="2. 内容构成"></a>2. 内容构成</h5><p>主要包括两部分内容，一部分是特征数据，另一部分是监督信息</p><ul><li><p>特征数据</p><ul><li><NumInputs>代表输入的特征类型，如果为2代表输入为特征+ivector。</li><li>神经网络的最小输入单位为一个chunk，由多帧构成，frames per chunk由自己设置，代表每一个chunk中含有30或37或50帧音频。</li><li>由于tdnn网络需要上下文信息，因此一个chunk中还需要输入left和right的context。</li></ul><p><img src="/2022/06/01/egs_in_kaldi/chunk.png"></p></li></ul><p><em>以pybind中的egs文件为例，rows 代表29 + 3 * 50 + 29 &#x3D; 208， cols  &#x3D;  40 * 3 (40维度&#x3D;mfcc特征+2阶delta)&#x3D;120</em></p><ul><li>监督信息（supervisions）<ul><li><NumSequences> : 这条ark文件中包含的chunk的个数，在chain模型中这个值为1，在pytorch版本中的kaldi这个值与minibatch_size一致。</li><li><FramesPerSeq>: 每chunk（sequence）包含的帧数，监督信息中的内容与对应这些帧数对应。</li><li><LabelDim>  : 决策树节点的个数，神经网络的输出节点个数</li><li>FST信息：chain模型训练需要分子有限状态机和分母有限状态机，这里的信息就是这个chunk所对应的分子有限状态机。它从lat.*.gz中来，是一种compact lattice，五列数值分别对应的是，开始状态，结束状态，输入trans-id ,输出trans-id，（有的状态含有权重）。</li></ul></li></ul><h3 id="3-kaldi-pybind中egs的构建过程"><a href="#3-kaldi-pybind中egs的构建过程" class="headerlink" title="3.kaldi-pybind中egs的构建过程"></a>3.kaldi-pybind中egs的构建过程</h3><ol><li>Raw_egs: 音频按照chunk的需要分成碎片<ul><li>每个音频索引上可以体现出选取的是音频的哪集帧</li><li>一条ark文件代表一个chunk</li></ul></li><li>Process_egs: 按照speaker将chunks合并，并按比例挑出训练集。然后局部打乱<ul><li>每一条ark文件在组成上与上一步没有什么区别</li><li>主要是将数据按照说话人聚集在一起 并且分出heldout_subset ，train_set 两个子集中的说话人也尽量分散</li></ul></li><li>Randomrize_egs：将ark文件全局打乱<ul><li>本质上只是对索引进行打乱，不生成新的文件</li></ul></li><li>merge_egs: 将minibatch_size个数的chunk进行合并<ul><li>此时的一条ark中包含多个chunk，也就是一个batch</li></ul></li><li>Align_egs:  <ul><li>为了ddp的计算，使得每个scp文件中的batch个数一致，会对齐每个scp文件的条目个数。</li></ul></li></ol><p>总结以上的过程，先生成多个chunk，合并minibatch_size个chunk为一个batch&#x3D;&gt; 把这些batch均匀的分给每个workers, 每个workers分到的batch就称为伪epoch&#x3D;&gt;多个伪epoch组成一个完整的epoch。</p>]]></content>
    
    
    
    <tags>
      
      <tag>egs</tag>
      
      <tag>pybind</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kaldi中的tdnn-f网络</title>
    <link href="/2022/06/01/tdnn-f_network_in_kaldi/"/>
    <url>/2022/06/01/tdnn-f_network_in_kaldi/</url>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><div class="code-wrapper"><pre><code class="hljs">TDNN-F（F stands for Factorization)本质上是一种改进的TDNN(Time Delay Neural Networks)网络。TDNN网络实际上是最早的CNN网络，可以看作是1d-conv。Kaldi中的TDNN网络考虑了语音序列的上下文特性，并在每一层采用跳跃拼帧的方式降低实际的计算量，如图1所示，每一个单元为一帧，横轴代表时间序列，纵轴代表层深的递进。</code></pre></div><p>​<img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190712316.png"></p><img src="tdnn-f_network_in_kaldi/20201128190712316.png"><p>​<strong>图一</strong> TDNN结构</p><p>   Tdnn-f的改进之处有两个方面，第一是权重矩阵的分解，第二是增加残差连接。权重矩阵分解的目的是减少模型的参数两，具体实现是将一个大的权重矩阵，分解成两个小的权重矩阵，其中一个矩阵满足半正定约束。这样在减少模型参数的同时，还能一定程度上维持模型的建模能力，dan的论文中对于factorize如图二所示。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190739762.png"></p><p>​<strong>图二 factorized的含义</strong></p><p>因此在一层tdnnf中，存在一个瓶颈结构，类似于res-net结构，如图三。</p><p>​<img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190812704.png"></p><p>​<strong>图三 resnet结构</strong></p><h2 id="1-kaldi中的tdnn-f"><a href="#1-kaldi中的tdnn-f" class="headerlink" title="1. kaldi中的tdnn-f"></a>1. kaldi中的tdnn-f</h2><p>在kaldi的源代码中，对于一层tdnn-f网络的构成解释如下图四，假设这一层的输入是tdnnf5的output，那么tdnnf6实际上是由一个linear层（与左-3时刻拼帧，节点维度由1024变成128），一个affine层（与右3帧拼帧，节点由128变成1024），再加上残差连接层（bypass值为0.66）构成的，如图5所示。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190825342.png"></p><p>​<strong>图</strong>4 </p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190839505.png"></p><p>​<strong>图5</strong></p><p>先不提残差连接的部分，tdnn结构中时间序列的变化可以用图6中所示表示，横轴体现的是以帧为单位的时间序列，纵轴是逐渐深入的层深。从最底层到最上层的连接关系的角度来看，左右两侧可以进行等价，这也就解释了kaldi中time-stride为3的含义。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190907391.png"></p><p>​<strong>图6    time-stride的含义</strong></p><p>kaldi中利用config文件对网络模型进行配置，一个12层的tdnn-f网络配置如下图7。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190925641.png"></p><p>​<strong>图7 tdnnf的config文件</strong></p><p>对于t&#x3D;0时刻的某一帧来说，送入这个网络需要提供相邻帧的上下文信息，也就是所谓的context，对于这样一个网络来说，左侧和右侧的context等于time-stride的加和，如下图8。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128190943348.png"></p><p>​<strong>图8context的含义</strong></p><p>可以看到，kaldi中的tdnnf也类似于1d-cnn，其中的time-stride含义类似于dilation的值，除了time-stride等于0的层，kernel的值都是3，stride值都是1。</p><h2 id="2-pytorch实现的tdnnf"><a href="#2-pytorch实现的tdnnf" class="headerlink" title="2. pytorch实现的tdnnf"></a>2. pytorch实现的tdnnf</h2><p>pytorch-y kaldi中实现的tdnn-f如下图9所示，它本质上是两个1d-cnn。除了节点个数和bypass值以外，这里开放的参数是第一个cnn（半正定层）的kernel_size，以及第二个cnn的stride,也就是subsample_factor。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128191023576.png"></p><p>​<strong>图9 实现结构</strong></p><p>为了与kaldi中的tdnnf进行对比，在相同context的情况下，用蓝色的线表示pytorch-y kaldi中tdnnf的前向过程，如下图10。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/2020112819104640.png"></p><p>​<strong>图10 两种实现的对比</strong></p><p>这种方式中的context不像kaldi中time-stride那么直观，但是可以通过cnn中的感受野计算公式来进行计算。但是这是要把每层tdnn-f理解成stride&#x3D;subsampling factor的kernel size的一个1d-cnn。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128191109210.png"></p><p>其中$r_n$代表每一层的感受野，k代表kernel的值，$s_i$代表每一层的stride。最终得到的感受野等于左context+当前帧+右context，如下图11。</p><p><img src="/2022/06/01/tdnn-f_network_in_kaldi/20201128191121734.png"></p><p>​<strong>图11 感受野的计算</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>tdnn-f</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
